{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dto-uOuWSZIE"
      },
      "source": [
        "# Mask R-CNN - Train pods dataset\n",
        "\n",
        "\n",
        "This notebook shows how to train Mask R-CNN implemented on coco on your own dataset. I trained the model to segment pods objects in an image. You'd need a GPU, because the network backbone is a Resnet101, which would be too slow to train on a CPU. The code is executable on google colaboratory GPU. On google colab you can start to get okay-ish results in a few minutes, and good results in less than an hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEbCSM2KsVS9"
      },
      "outputs": [],
      "source": [
        "!rm -rf pods_dataset/\n",
        "!rm -rf sample_data/\n",
        "!rm -rf logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9IIRSdUcbW2"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==1.13.1\n",
        "!pip install keras==2.2.5\n",
        "!pip install imgaug==0.4.0\n",
        "!pip install scikit-image==0.16.2\n",
        "!pip install h5py==2.10.0\n",
        "!pip install numpy==1.18.5\n",
        "!pip install mrcnn-colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUv2fZL5WgFN"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/barbaraport/pods_dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaHtb9hysSPJ"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8eAPTNTuTYD"
      },
      "outputs": [],
      "source": [
        "!pip show keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPQLeVZNoofS"
      },
      "outputs": [],
      "source": [
        "!pip show tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMwapxaFSZIH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pandas as pd\n",
        "import skimage\n",
        "\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "\n",
        "from mrcnn.config import Config\n",
        "import mrcnn.utils as utils\n",
        "import mrcnn.model as modellib\n",
        "import mrcnn.visualize as visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "# Data Path\n",
        "TRAIN_PATH = 'pods_dataset/trainData/stage2_train/'\n",
        "TEST_PATH = 'pods_dataset/trainData/stage2_test/'\n",
        "\n",
        "# Get train and test IDs\n",
        "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
        "test_ids = next(os.walk(TEST_PATH))[1]\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.getcwd()\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpT9HgC7SZIN"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBONWUhASZIO"
      },
      "outputs": [],
      "source": [
        "class ShapesConfig(Config):\n",
        "    \"\"\"Configuration for training on the dataset.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the dataset.\n",
        "    \"\"\"\n",
        "    BACKBONE = \"resnet50\"\n",
        "\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"shapes\"\n",
        "\n",
        "    # Train on 1 GPU and 1 images per GPU. We can put multiple images on each\n",
        "    # GPU. Batch size is (GPUs * images/GPU).\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "    # Steps per epoch \n",
        "    STEPS_PER_EPOCH = 500\n",
        "\n",
        "    # Image resize mode\n",
        "    # No changes to the image\n",
        "    IMAGE_RESIZE_MODE = \"none\"\n",
        "    IMAGE_MAX_DIM = 1024\n",
        "    IMAGE_MIN_DIM = 1024\n",
        "\n",
        "    # Minimum probability value to accept a detected instance\n",
        "    # ROIs below this threshold are skipped\n",
        "    DETECTION_MIN_CONFIDENCE = 0.6\n",
        "\n",
        "    # Non-maximum suppression threshold for detection\n",
        "    DETECTION_NMS_THRESHOLD = 0.1\n",
        "\n",
        "    # Length of square anchor side in pixels\n",
        "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "\n",
        "    # Non-max suppression threshold to filter RPN proposals.\n",
        "    # You can increase this during training to generate more proposals.\n",
        "    RPN_NMS_THRESHOLD = 0.1\n",
        "\n",
        "    \n",
        "config = ShapesConfig()\n",
        "config.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dhvNUELSZIS"
      },
      "source": [
        "## Notebook Preferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj3zh7wMSZIW"
      },
      "outputs": [],
      "source": [
        "def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjEu-7I1SZIY"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Create a synthetic dataset\n",
        "\n",
        "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
        "\n",
        "* load_image()\n",
        "* load_mask()\n",
        "* image_reference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqkR3-OHSZIY"
      },
      "outputs": [],
      "source": [
        "class ShapesDataset(utils.Dataset):\n",
        "    \n",
        "    def load_shapes(self, mode, is_train=True):\n",
        "        self.add_class(\"shapes\", 1, \"pod\")\n",
        "        \n",
        "        images_dir = TRAIN_PATH + \"images/\"\n",
        "        annotations_dir = TRAIN_PATH + \"annotations/\"\n",
        "\n",
        "        if not is_train:\n",
        "            images_dir = TEST_PATH + \"images/\"\n",
        "            annotations_dir = TEST_PATH + \"annotations/\"\n",
        "\n",
        "        filenames = os.listdir(images_dir)\n",
        "        files_quantity = len(filenames)\n",
        "\n",
        "        for i in range(files_quantity):\n",
        "            filename = filenames[i]\n",
        "            image_id = i\n",
        "            \n",
        "            image_path = images_dir + filename\n",
        "            annotation_path = annotations_dir + filename[:-4] + '.json'\n",
        "\n",
        "            annotation = json.load(open(os.path.join(annotation_path)))\n",
        "\n",
        "            shapes = [] \n",
        "            class_ids = []\n",
        "            labels_list = []\n",
        "\n",
        "            for shape in annotation[\"shapes\"]:\n",
        "                label = shape[\"label\"]\n",
        "                if labels_list.count(label) == 0:\n",
        "                    labels_list.append(label)\n",
        "                class_ids.append(labels_list.index(label)+1)\n",
        "                points = shape[\"points\"]\n",
        "                shapes.append(points)\n",
        "            \n",
        "            width = annotation[\"imageWidth\"]\n",
        "            height = annotation[\"imageHeight\"]\n",
        "            \n",
        "            self.add_image('shapes', image_id = image_id, path = image_path, annotation = annotation_path, width = width, height = height, shapes = shapes, class_ids = class_ids)\n",
        "            i += 1   \n",
        "\n",
        "    def load_image(self, image_id):\n",
        "        \n",
        "        info = self.image_info[image_id]\n",
        "        path = info.get(\"path\")\n",
        "\n",
        "        img = imread(path)[:,:,:3]\n",
        "        img = resize(img, (config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1]), mode='constant', preserve_range=True)\n",
        "       \n",
        "        return img\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"shapes\":\n",
        "            return info[\"shapes\"]\n",
        "        else:\n",
        "            super(self.__class__).image_reference(self, image_id)\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"shapes\"])], dtype=np.uint8)\n",
        "\n",
        "        for idx, points in enumerate(info[\"shapes\"]):\n",
        "            pointsy, pointsx = zip(*points)\n",
        "            rr, cc = skimage.draw.polygon(pointsx, pointsy)\n",
        "            mask[rr, cc, idx] = 1\n",
        "\n",
        "        masks_np = mask.astype(np.bool)\n",
        "        classids_np = np.array(info[\"class_ids\"]).astype(np.int32)\n",
        "        \n",
        "        return masks_np, classids_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAhNtW_TSZIb"
      },
      "outputs": [],
      "source": [
        "# Training dataset\n",
        "dataset_train = ShapesDataset()\n",
        "dataset_train.load_shapes('shapes', is_train=True)\n",
        "dataset_train.prepare()\n",
        "print('Train: %d' % len(dataset_train.image_ids))\n",
        "\n",
        "# Validation dataset\n",
        "dataset_val = ShapesDataset()\n",
        "dataset_val.load_shapes('shapes', is_train=False)\n",
        "dataset_val.prepare()\n",
        "print('Validation: %d' % len(dataset_val.image_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAizzQC5SZIf"
      },
      "outputs": [],
      "source": [
        "# Load and display random samples\n",
        "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
        "for image_id in image_ids:\n",
        "    image = dataset_train.load_image(image_id)\n",
        "    mask, class_ids = dataset_train.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyEQKAnMSZIi"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GCYogpmSZIj"
      },
      "outputs": [],
      "source": [
        "# Create model in training mode\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,model_dir=MODEL_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUuC-fI4SZIl"
      },
      "outputs": [],
      "source": [
        "# Which weights to start with?\n",
        "init_with = \"last\"  # imagenet, coco, or last\n",
        "\n",
        "if init_with == \"imagenet\":\n",
        "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
        "elif init_with == \"coco\":\n",
        "    model.load_weights(\"mask_rcnn_coco.h5\", by_name=True,\n",
        "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "elif init_with == \"last\":\n",
        "    # Load the last model you trained and continue training\n",
        "    model.load_weights(\"mask_rcnn_shapes_0006.h5\", by_name=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzZuQxylSZIo"
      },
      "source": [
        "## Training\n",
        "\n",
        "Train in two stages:\n",
        "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
        "\n",
        "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCuJi9Cn9DzX"
      },
      "outputs": [],
      "source": [
        "augmentation = iaa.OneOf([\n",
        "    iaa.Crop(px=(0, 16)),\n",
        "    iaa.Fliplr(1.0),\n",
        "    iaa.Flipud(1.0),\n",
        "    iaa.Affine(\n",
        "        rotate=(45, -45),\n",
        "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
        "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2jmfqdLSZIp"
      },
      "outputs": [],
      "source": [
        "model.train(dataset_train, dataset_val,\n",
        "    learning_rate=config.LEARNING_RATE/ 10,\n",
        "    epochs=6,\n",
        "    layers='heads',\n",
        "    augmentation=augmentation\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(dataset_train, dataset_val,\n",
        "    learning_rate=config.LEARNING_RATE / 10,\n",
        "    epochs=10,\n",
        "    layers='all',\n",
        "    augmentation=augmentation\n",
        ")"
      ],
      "metadata": {
        "id": "quEQ2IzJauP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG8lGckkODGY"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard.notebook\n",
        "%load_ext tensorboard.notebook\n",
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldulTQXRSZI0"
      },
      "source": [
        "## Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWhUidKESZI1"
      },
      "outputs": [],
      "source": [
        "class InferenceConfig(ShapesConfig):\n",
        "    NAME = \"eS_inference\"\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "inference_config.display()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode=\"inference\", \n",
        "                          config=inference_config,\n",
        "                          model_dir=MODEL_DIR)\n",
        "\n",
        "# Get path to saved weights\n",
        "# Either set a specific path or find last trained weights\n",
        "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
        "# model_path = model.find_last()[1]\n",
        "model_path = \"mask_rcnn_shapes_0004.h5\"\n",
        "\n",
        "# Load trained weights (fill in path to trained weights here)\n",
        "assert model_path != \"\", \"Provide path to trained weights\"\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "metadata": {
        "id": "_mC2zbO7uB2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-aJVKAhSZI3"
      },
      "outputs": [],
      "source": [
        "test_image = \"pods_dataset/trainData/stage2_train/images/1fd56af5-befa-4bdd-871e-15edb7523cad_1.jpg\" # Choose random or provide an image path\n",
        "\n",
        "if test_image == \"random\":\n",
        "    # Test on a random image\n",
        "    image_id = random.choice(dataset_val.image_ids)\n",
        "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config, \n",
        "                            image_id, use_mini_mask=False)\n",
        "        \n",
        "    log(\"original_image\", original_image)\n",
        "    log(\"image_meta\", image_meta)\n",
        "    log(\"gt_class_id\", gt_class_id)\n",
        "    log(\"gt_bbox\", gt_bbox)\n",
        "    log(\"gt_mask\", gt_mask)\n",
        "\n",
        "    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, dataset_train.class_names, figsize=(8, 8))\n",
        "else:\n",
        "    assert test_image != \"\", \"Choose the type of test\"\n",
        "    assert test_image != \"random\", \"Please, provide an image path\"\n",
        "    original_image = cv2.imread(test_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyAT1glESZI7"
      },
      "outputs": [],
      "source": [
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8mN9BsDSZI-"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib5pJXg3SZJA"
      },
      "outputs": [],
      "source": [
        "# Compute VOC-Style mAP\n",
        "# Increase images quantity for better accuracy.\n",
        "def calculate_mAP(dataset, image_ids):\n",
        "    APs = []\n",
        "\n",
        "    for image_id in image_ids:\n",
        "        # Load image and ground truth data\n",
        "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "            modellib.load_image_gt(dataset, inference_config,\n",
        "                                image_id, use_mini_mask=False)\n",
        "        molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
        "        # Run object detection\n",
        "        results = model.detect([image], verbose=0)\n",
        "        r = results[0]\n",
        "        # Compute AP\n",
        "        AP, precisions, recalls, overlaps =\\\n",
        "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
        "                            r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'], iou_threshold=0.1)\n",
        "        APs.append(AP)\n",
        "    return APs\n",
        "\n",
        "image_val_ids = dataset_val.image_ids\n",
        "image_val_APs = calculate_mAP(dataset_val, image_val_ids)\n",
        "\n",
        "image_train_ids = dataset_train.image_ids\n",
        "image_train_APs = calculate_mAP(dataset_train, image_train_ids)\n",
        "\n",
        "APs = np.concatenate((image_train_APs, image_val_APs))\n",
        "\n",
        "mAP = np.mean(APs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eetbs3ngxjRc"
      },
      "outputs": [],
      "source": [
        "print(\"mAP: \", mAP)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}